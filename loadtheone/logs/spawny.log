* input data: env at 0x43000000, 3830 bytes. [+29367]
* strings at 0x80000010, environ[63] at 0x80000f10. [+30417]
* heap at 0x80000000, 10737418240 bytes. [+351]
* input data: fibre at 0x80001140, 32 bytes. [+1479]
* input data: slr vars at 0x80001118, 16 bytes. [+866]
* connected to I/O, 16 devices, 8 notification channels. [+767]
* smc at 0x4e000000, enumerates 15 devices. [+513]
* I/O enumeration data at 0x10000000. [+1282]
* uart at 0x44000000. [+1217]
* rpc interface at 0x45000000. [+608]
* lcd at 0x46000000, size 80x25. [+453]
* lcd at 0x47000000, size 16x2. [+369]
* rtc at 0x48000000. [+306]
* gfxctl at 0x49000000, framebuffer at 0x4a000000. [+613]
* argument ROM at 0x4c000000. [+815]
* configuration ROM at 0x4d000000. [+527]
* input data: arguments at 0x80001168, argv[2] at 0x800011a8, 52 bytes. [+2977]
* reading configuration...
  - core frequency: 1000MHz.
  - 256 thread and 32 family entries per core.
  - system version: 3.2.61-d2aa8.
  - master frequency: 4000MHz.
  done. [+244807]
* SEP init: setting up for 128 cores...  done. [+2685]
* allocating places for the C stdlib... success: stdio at 0x103, malloc at 0x105, dtoa at 0x107 [+2658]
* -n 1 -> request to SEP... success, t_main goes to pid=0x109 [+7703]
* init compiled with slc 3.7b.40-c437a2.

Microgrid says:  h e l l o  w o r l d !

init done, program will start now; core cycles so far: 348997
Program from config ./cfg/spawny.cfg
Argroom needed 61, 16384 available
Envroom needed 5, 16384 available

Arg
Returning from Loader main

How

How

Be

<Clocks>1,0,1,499492,130855,455714,456014</Clocks>

Be

Be

Ye

Ye

Ye

Be

Matey

Ye

Matey

Ye

Ye

Ye

Matey

Matey

Matey

Matey

Matey

Matey

Matey

Matey

Matey

Ye

Matey

Matey

Matey

Matey

Matey
### begin end-of-simulation statistics
23762704	# master cycle counter
5940676	# core cycle counter
3315870	# total executed instructions
0	# total issued fp instructions
## core statistics:
# P  iops      flops     lds       sts       ibytes    obytes    regf_act plbusy    plstgs    plstgrun  pl%busy  pl%eff   ttmax     ttotal    ttsize    tt%occ   ftmax     ftotal    ftsize    ft%occ   xqmax     xqtot     xqavg        
# per-core values
0    3292896   0         211202    707531    1231114   3104302   1.3      4011671   6         20282809  84.3     55.4     25        209891132 256       3.5      25        210353716 32        27.7     13        71305092  3.000714e+00 
1    6403      0         1018      622       5495      3334      0.0      13261     6         41610     52.3     0.1      1         130048    256       0.0      1         131500    32        0.0      1         3827056   1.610530e-01 
2    13862     0         2149      1638      13684     12644     0.0      28865     6         90183     52.1     0.2      1         260784    256       0.0      1         309820    32        0.0      17        25007396  1.052380e+00 
3    2709      0         315       504       1764      2520      0.0      6890      6         19173     46.4     0.0      1         112300    256       0.0      1         207656    32        0.0      8         23856680  1.003955e+00 
# cumulative - all active cores
4    3315870   0         214684    710295    1252057   3122800   1.4      4060687   24        20433775  235.0    55.8     28        210394264 1024      3.5      28        211002692 128       27.7     39        123996224 5.218102e+00 
# average per core = cumulative/4
4    828967.50 0.00      53671.00  177573.75 313014.25 780700.00 0.3      1015171.75 6.00      5108444.00 58.8     14.0     7.00      52598568.00 256.00    0.9      7.00      52750672.00 32.00     6.9      9.75      30999056.00 1.304526e+00 
## descriptions:
# P: core ID / number of active cores
# iops: number of instructions executed
# flops: number of floating-point instructions issued
# lds: number of load instructions executed
# sts: number of store instructions executed
# ibytes: number of bytes loaded from L1 cache into core
# obytes: number of bytes stored into L1 cache from core
# regf_act: register file async port activity (= 100. * ncycles_asyncport_busy / ncorecycles_total)
# plbusy: number of corecycles the pipeline was active
# plstgs: number of pipeline stages
# plstgrun: cumulative number of corecycles active in all pipeline stages
# pl%busy: pipeline efficiency while active (= 100. * plstgrun / plstgs / plbusy)
# pl%eff: pipeline efficiency total (= 100. * iops / ncorecycles_total)
# ttmax: maximum of thread entries simulatenously allocated
# ttotal: cumulative number of thread entries busy (over mastertime)
# ttsize: thread table size
# tt%occ: thread table occupancy (= 100. * ttotal / ttsize / nmastercycles_total)
# ttmax: maximum of family entries simulatenously allocated
# ftotal: cumulative number of family entries busy (over mastertime)
# ftsize: family table size
# ft%occ: family table occupancy (= 100. * ftotal / ftsize / nmastercycles_total)
# xqmax: high water mark of the exclusive allocate queue size
# xqtot: cumulative exclusive allocate queue size (over mastertime)
# xqavg: average size of the exclusive allocate queue (= xqtot / nmastercycles_total)
## memory statistics:
30935	# number of load reqs. by the L1 cache from L2
1979840	# number of bytes loaded by the L1 cache from L2
743843	# number of store reqs. by the L1 cache to L2
7422762	# number of bytes stored by the L1 cache to L2
134878	# number of cache lines read from the ext. mem. interface
121389	# number of cache lines written to the ext. mem. interface
### end end-of-simulation statistics
